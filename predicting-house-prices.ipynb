{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Predicting House Prices**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Introduction\n\nThis is a Kaggle Machine Learning competition designed for Kaggle Learn users. The goal is to predict house prices based on a variety of features provided in the training and test datasets. We'll use multiple regression models to make predictions and evaluate which model performs best.\n\nSince the test dataset does not include the `SalePrice` column (as expected in most real-world scenarios), we will simulate this setup by splitting the original training data into **training and validation subsets**. This allows us to train models on one portion and evaluate their performance on unseen validation data before making final predictions on the actual test set.\n\nThe dataset includes nearly 80 columns, so identifying the most relevant features that impact the sale price is essential. We'll start by exploring the data, understanding the meaning of each column, and handling missing values. Any columns with excessive missing data will be dropped to maintain data quality.\n\nFeature engineering will be a key part of this project. It helps us avoid issues like overfitting or underfitting. Once we build and evaluate our models using RMSE (Root Mean Squared Error), we’ll refine our features and finalize the best model for prediction.\n\nLet’s get started by diving into the data!","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries and Dataset\nIn this project, we’ll be using multiple regression models such as Lasso, Ridge, Decision Tree, Random Forest, and XGBoost to predict house prices. To support data preprocessing, model training, and evaluation, we’ll primarily rely on libraries from Scikit-learn `sklearn`, along with a few other essential Python libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd #data manipulation\nimport numpy as np #numerical analysis\nimport matplotlib.pyplot as plt #basic data visualization\nimport seaborn as sns #advanced statistical visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T04:42:32.275286Z","iopub.execute_input":"2025-04-08T04:42:32.275601Z","iopub.status.idle":"2025-04-08T04:42:32.282209Z","shell.execute_reply.started":"2025-04-08T04:42:32.275575Z","shell.execute_reply":"2025-04-08T04:42:32.281059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great! Now as we got our required libraries it's time to load data sets. As the data is available in the kaggle repository itself we have just import from there itself.","metadata":{}},{"cell_type":"code","source":"#import the data set and read them into pandas data frame\nfile_path_train = '/kaggle/input/home-data-for-ml-course/train.csv'\nfile_path_test = '/kaggle/input/home-data-for-ml-course/test.csv'\n\ntrain_df = pd.read_csv(file_path_train)\ntest_df = pd.read_csv(file_path_test)\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T04:42:34.718245Z","iopub.execute_input":"2025-04-08T04:42:34.718555Z","iopub.status.idle":"2025-04-08T04:42:34.778558Z","shell.execute_reply.started":"2025-04-08T04:42:34.718530Z","shell.execute_reply":"2025-04-08T04:42:34.777332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great! we got our train and test data sets loaded, it's time to analyze the data, let's have a look at each column and undersatand what it actually ment, this help us to think logically as well when performing feature engineering. \n\nNote - We do have data description file in text format, make sure to read it thouroughly to understand the data, it is very important, because if we don't know what we're doing the there's no point of doing it! lol!!!","metadata":{}},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## House Prices Dataset - Feature Description\n\n| S.No | Column         | Description                                                  |\n|------|----------------|--------------------------------------------------------------|\n| 1    | SalePrice      | The property's sale price in dollars. Target variable.       |\n| 2    | MSSubClass     | The building class                                           |\n| 3    | MSZoning       | The general zoning classification                            |\n| 4    | LotFrontage    | Linear feet of street connected to property                  |\n| 5    | LotArea        | Lot size in square feet                                      |\n| 6    | Street         | Type of road access                                          |\n| 7    | Alley          | Type of alley access                                         |\n| 8    | LotShape       | General shape of property                                    |\n| 9    | LandContour    | Flatness of the property                                     |\n| 10   | Utilities      | Type of utilities available                                  |\n| 11   | LotConfig      | Lot configuration                                            |\n| 12   | LandSlope      | Slope of property                                            |\n| 13   | Neighborhood   | Physical locations within Ames city limits                   |\n| 14   | Condition1     | Proximity to main road or railroad                           |\n| 15   | Condition2     | Proximity to main road or railroad (if a second is present)  |\n| 16   | BldgType       | Type of dwelling                                             |\n| 17   | HouseStyle     | Style of dwelling                                            |\n| 18   | OverallQual    | Overall material and finish quality                          |\n| 19   | OverallCond    | Overall condition rating                                     |\n| 20   | YearBuilt      | Original construction date                                   |\n| 21   | YearRemodAdd   | Remodel date                                                 |\n| 22   | RoofStyle      | Type of roof                                                 |\n| 23   | RoofMatl       | Roof material                                                |\n| 24   | Exterior1st    | Exterior covering on house                                   |\n| 25   | Exterior2nd    | Exterior covering on house (if more than one material)       |\n| 26   | MasVnrType     | Masonry veneer type                                          |\n| 27   | MasVnrArea     | Masonry veneer area in square feet                           |\n| 28   | ExterQual      | Exterior material quality                                    |\n| 29   | ExterCond      | Present condition of the material on the exterior            |\n| 30   | Foundation     | Type of foundation                                           |\n| 31   | BsmtQual       | Height of the basement                                       |\n| 32   | BsmtCond       | General condition of the basement                            |\n| 33   | BsmtExposure   | Walkout or garden level basement walls                       |\n| 34   | BsmtFinType1   | Quality of basement finished area                            |\n| 35   | BsmtFinSF1     | Type 1 finished square feet                                  |\n| 36   | BsmtFinType2   | Quality of second finished area (if present)                 |\n| 37   | BsmtFinSF2     | Type 2 finished square feet                                  |\n| 38   | BsmtUnfSF      | Unfinished square feet of basement area                      |\n| 39   | TotalBsmtSF    | Total square feet of basement area                           |\n| 40   | Heating        | Type of heating                                              |\n| 41   | HeatingQC      | Heating quality and condition                                |\n| 42   | CentralAir     | Central air conditioning                                     |\n| 43   | Electrical     | Electrical system                                            |\n| 44   | 1stFlrSF       | First Floor square feet                                      |\n| 45   | 2ndFlrSF       | Second floor square feet                                     |\n| 46   | LowQualFinSF   | Low quality finished square feet (all floors)                |\n| 47   | GrLivArea      | Above grade (ground) living area square feet                 |\n| 48   | BsmtFullBath   | Basement full bathrooms                                      |\n| 49   | BsmtHalfBath   | Basement half bathrooms                                      |\n| 50   | FullBath       | Full bathrooms above grade                                   |\n| 51   | HalfBath       | Half baths above grade                                       |\n| 52   | Bedroom        | Number of bedrooms above basement level                      |\n| 53   | Kitchen        | Number of kitchens                                           |\n| 54   | KitchenQual    | Kitchen quality                                              |\n| 55   | TotRmsAbvGrd   | Total rooms above grade (excluding bathrooms)                |\n| 56   | Functional     | Home functionality rating                                    |\n| 57   | Fireplaces     | Number of fireplaces                                         |\n| 58   | FireplaceQu    | Fireplace quality                                            |\n| 59   | GarageType     | Garage location                                              |\n| 60   | GarageYrBlt    | Year garage was built                                        |\n| 61   | GarageFinish   | Interior finish of the garage                                |\n| 62   | GarageCars     | Size of garage in car capacity                               |\n| 63   | GarageArea     | Size of garage in square feet                                |\n| 64   | GarageQual     | Garage quality                                               |\n| 65   | GarageCond     | Garage condition                                             |\n| 66   | PavedDrive     | Paved driveway                                               |\n| 67   | WoodDeckSF     | Wood deck area in square feet                                |\n| 68   | OpenPorchSF    | Open porch area in square feet                               |\n| 69   | EnclosedPorch  | Enclosed porch area in square feet                           |\n| 70   | 3SsnPorch      | Three season porch area in square feet                       |\n| 71   | ScreenPorch    | Screen porch area in square feet                             |\n| 72   | PoolArea       | Pool area in square feet                                     |\n| 73   | PoolQC         | Pool quality                                                 |\n| 74   | Fence          | Fence quality                                                |\n| 75   | MiscFeature    | Miscellaneous feature not covered in other categories        |\n| 76   | MiscVal        | $Value of miscellaneous feature                              |\n| 77   | MoSold         | Month Sold                                                   |\n| 78   | YrSold         | Year Sold                                                    |\n| 79   | SaleType       | Type of sale                                                 |\n| 80   | SaleCondition  | Condition of sale                                            |\n| 81   | ID             | Index                                                        \n\nNote - Read 'data_description.txt' file for full understanding of the features","metadata":{}},{"cell_type":"markdown","source":"Wow, it’s amazing how much there is to learn! Until now, I had no idea that so many features are considered when determining the value of a house. This makes the project even more exciting as we dig deeper into the data.\nI’m especially interested in analyzing how different features impact the sale price—understanding this could be incredibly valuable when I’m working with a real estate agent in the future. With this knowledge, I’ll be in a much better position to evaluate homes and negotiate a fair price for my dream house.","metadata":{}},{"cell_type":"markdown","source":"As we can see our tarin data frame, there are some missing values with NaN, so first let's have a look at each column that have missing values and analyze what it is, before that let's have a quick, that how many data points does our data set have.","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T04:42:44.279367Z","iopub.execute_input":"2025-04-08T04:42:44.279690Z","iopub.status.idle":"2025-04-08T04:42:44.286666Z","shell.execute_reply.started":"2025-04-08T04:42:44.279664Z","shell.execute_reply":"2025-04-08T04:42:44.285448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have 1460 rows and 81 columns","metadata":{}},{"cell_type":"code","source":"missing_values = train_df.isnull().sum()\nmissing_values = missing_values[missing_values > 0]\nmissing_values = missing_values.sort_values(ascending = False)\nprint(missing_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T04:49:21.050684Z","iopub.execute_input":"2025-04-08T04:49:21.051068Z","iopub.status.idle":"2025-04-08T04:49:21.067445Z","shell.execute_reply.started":"2025-04-08T04:49:21.051039Z","shell.execute_reply":"2025-04-08T04:49:21.065926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ohh! okay, we have in total 19 columns with missing values among them columns `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `ManVnrType`, `FirePlaceQC`, `LotFrontage` have many missing values. Now let's analyze one by one\n\n1. `PoolQC` - This column represents the quality of pool, bu the results show among 1460 houses 1453 house doen't have pool. So now if we replace these missing values with most frequent value of rest 7 houses will impact the sale price very badly and predict wrong results. So let's drop out the 'poolqc' column.\n2. `MiscFeature` - This column represents any other features home has, like elevator and other features. Results show we have 1406 missing values among 1460 houses. So let's drop the this column as well.\n3. `Alley` - this columns represents the passage to backside of the home, result shows 1369/1460 houses doesn't have alley. So let's drop this column as well\n4. `Fence` - 1179/1460 missing values\n5. `MasVnrType`  - 872/1460 missing values\n6. `FireplaceQu` - 690/1460 missing values","metadata":{}},{"cell_type":"markdown","source":"Now that we've identified the columns to drop, it's time to split the dataset into training and validation subsets for model evaluation.  \n\nBut before we do that, let's take a closer look at the distribution of the **SalePrice** column — our target variable. In housing data, the sale price is often **right-skewed**, meaning there are a few extremely high-priced properties that can distort the overall distribution.\n\nTo ensure better model performance, especially for linear models, it's important to **normalize the target variable**. We’ll examine the distribution and apply a log transformation if needed to reduce skewness and bring the data closer to a normal distribution.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\n\nplt.figure(figsize = (10,4))\nsns.histplot(train_df['SalePrice'], kde = True, bins = 30)\nplt.title('Distribution of Sale Price')\nplt.xlabel('Sale Price')\nplt.ylabel('Frequency')\nplt.show()\n\n#calculating skewness\nsalesprice_skew = skew(train_df['SalePrice'])\nprint(f\"Sale Price Skewness: {salesprice_skew: .2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T05:25:13.831691Z","iopub.execute_input":"2025-04-08T05:25:13.832102Z","iopub.status.idle":"2025-04-08T05:25:14.477289Z","shell.execute_reply.started":"2025-04-08T05:25:13.832071Z","shell.execute_reply":"2025-04-08T05:25:14.476049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Yoo! So our sales price data is right skewed with most of the houses range between 100000 to 200000 dollars. \n\nAs the house prices are not normally distributed — right-skewed, meaning:\n1. Most homes are moderately priced\n2. A few are extremely expensive (outliers)\n3. The distribution is not symmetric\n\nMachine learning models (especially linear ones like Ridge or Lasso) work better when the target variable is closer to a normal distribution. So, let perform log transformation `np.log1p(SalePrice)` to reduces the effect of outliers, helps models learn better relationships, and improves RMSE and overall model performance\n\n\nInterpretation:\nSkewness = 0 → Perfectly normal distribution\nSkewness > 0 → Right-skewed (long tail on the right)\nSkewness < 0 → Left-skewed","metadata":{}},{"cell_type":"code","source":"#define X and y values to spilt\nX = train_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu', 'LotFrontage', 'SalePrice'], axis = 1)\ny = np.log1p(train_df['SalePrice'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:33.362896Z","iopub.execute_input":"2025-04-08T09:35:33.363332Z","iopub.status.idle":"2025-04-08T09:35:33.370967Z","shell.execute_reply.started":"2025-04-08T09:35:33.363297Z","shell.execute_reply":"2025-04-08T09:35:33.369655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's split the train data into train and validation subsets with the validation split of 30% and random state of 42","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:36.390073Z","iopub.execute_input":"2025-04-08T09:35:36.390466Z","iopub.status.idle":"2025-04-08T09:35:36.403405Z","shell.execute_reply.started":"2025-04-08T09:35:36.390430Z","shell.execute_reply":"2025-04-08T09:35:36.401972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Wonderful! We got our train and validation data sets. Before proceeding let's have a look at the categorical columns, these are the tricky one when performing machine learning models, Unlike our numeric variable these are not the easy ones. Machin leaning models require values in numbers. So let's encode them, mainly there are 3 types of encoding style 'OneHotEncoding', 'Label Encoding', and 'Ordinal Encoding'. Let's understand one by one\n\n1. `OneHotEncoding`: This splits the data from the column and add up new columns and assign the binary values of 0 and 1. This is best for linear models like (Ridg and lasso regression models)\n2. `Label Encoding`: This assigns the value to the unique data points in the column like 0, 1, 2, ...etc,. This is good for tree type regression models (Decision Tree, Random Forest, XGB)\n3. `Ordinal Encoding`: This assigns the value based on the ranking like Execllent > Good > Poor > worst. This is good for the ordinal category (real ranking)\n\nSo, for this project we will be focusing on One Hot Encoding for Ridge and Lasso regression models and label encoding for tree models.\n\nNow what about the numeric variable, numeric variables need ","metadata":{}},{"cell_type":"markdown","source":"# Model Development","metadata":{}},{"cell_type":"code","source":"#slipt the numeric and categorical variables \nnumeric_features = X.select_dtypes(include = ['number']).columns\ncategorical_features = X.select_dtypes(include = ['object', 'category']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:38.470661Z","iopub.execute_input":"2025-04-08T09:35:38.471043Z","iopub.status.idle":"2025-04-08T09:35:38.479240Z","shell.execute_reply.started":"2025-04-08T09:35:38.471013Z","shell.execute_reply":"2025-04-08T09:35:38.477354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preprocessing pipeline for linear models (Ridge and Lasso)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnumeric_features_pipeline = Pipeline(steps = [\n    ('Imputer', SimpleImputer(strategy = 'median')),\n    ('Scaler', StandardScaler())\n])\n\ncategorical_features_pipeline = Pipeline(steps = [\n    ('Imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('OneHot', OneHotEncoder(handle_unknown = 'ignore'))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:44.700458Z","iopub.execute_input":"2025-04-08T09:35:44.700806Z","iopub.status.idle":"2025-04-08T09:35:44.706607Z","shell.execute_reply.started":"2025-04-08T09:35:44.700777Z","shell.execute_reply":"2025-04-08T09:35:44.705148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#let's combine both the transformed feature pipelines into one using column transformer\npreprocessor = ColumnTransformer(transformers = [\n    ('numeric', numeric_features_pipeline, numeric_features),\n    ('categorical', categorical_features_pipeline, categorical_features)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:47.403681Z","iopub.execute_input":"2025-04-08T09:35:47.404127Z","iopub.status.idle":"2025-04-08T09:35:47.409256Z","shell.execute_reply.started":"2025-04-08T09:35:47.404097Z","shell.execute_reply":"2025-04-08T09:35:47.407862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a full modeling pipeline\n\nNow we’ll combine the preprocessor (for data cleaning and transformation) a regression model (Ridge). This way, everything from preprocessing to prediction happens in one go!","metadata":{}},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"model_pipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', Ridge())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:49.443197Z","iopub.execute_input":"2025-04-08T09:35:49.443596Z","iopub.status.idle":"2025-04-08T09:35:49.449874Z","shell.execute_reply.started":"2025-04-08T09:35:49.443564Z","shell.execute_reply":"2025-04-08T09:35:49.448488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model tuning and validation\nFor getting better performance and avoiding overfitting.\n\nNow instead of just .fit(), let's use:\n- Step 1: cross_val_score() – Quick performance estimate\n- Step 2: GridSearchCV – For best model & hyperparameter tuning\nLet’s go with Step 1 first for quick validation, and we’ll do Step 2 right after.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\n\n#use negative RMSE because sklearn minimizes the losses\nscores = cross_val_score(model_pipeline, X_train, y_train, cv = 5, \n                         scoring = 'neg_root_mean_squared_error')\n\n#print avg RMSE across folds\nprint('Avg RMSE:', -np.mean(scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:35:53.592058Z","iopub.execute_input":"2025-04-08T09:35:53.592423Z","iopub.status.idle":"2025-04-08T09:35:53.872407Z","shell.execute_reply.started":"2025-04-08T09:35:53.592397Z","shell.execute_reply":"2025-04-08T09:35:53.871161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hyperparameter tuning with GridSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'model__alpha': [0.01, 0.1, 1, 10, 50, 100]\n}\n\ngrid_search = GridSearchCV(model_pipeline, param_grid,\n                          cv = 5,\n                          scoring = 'neg_root_mean_squared_error',\n                          n_jobs = -1)\ngrid_search.fit(X_train, y_train)\n\n#print best param and score\nprint('Best Paramater:', grid_search.best_params_)\nprint('Bst CV RMSE:', -grid_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:00.273583Z","iopub.execute_input":"2025-04-08T09:36:00.274074Z","iopub.status.idle":"2025-04-08T09:36:02.908452Z","shell.execute_reply.started":"2025-04-08T09:36:00.274038Z","shell.execute_reply":"2025-04-08T09:36:02.906761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Yoo! we dit it, but is tis the best model? well we can answer that once we check the others. As we gone through step by step for the above problem let's quickly have a look at the next linear model that is lasso.\n\nNote we have to start from defining model pipeline.","metadata":{}},{"cell_type":"markdown","source":"## Lasso Regression","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nmodel_pipeline_lasso = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', Lasso(max_iter = 1000))\n])\n\n#model tunning and validation\nscores = cross_val_score(model_pipeline_lasso, X_train, y_train, cv = 6,\n                        scoring = 'neg_root_mean_squared_error')\n\nprint('Avg RMSE lasso: ', -np.mean(scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:07.200658Z","iopub.execute_input":"2025-04-08T09:36:07.201722Z","iopub.status.idle":"2025-04-08T09:36:07.492817Z","shell.execute_reply.started":"2025-04-08T09:36:07.201644Z","shell.execute_reply":"2025-04-08T09:36:07.491896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#now let's add hyperparameters and perform grid search\nparam_grid = {\n    'model__alpha': [0.01, 0.1, 1, 10, 50, 100]\n}\n\ngrid_search_lasso = GridSearchCV(model_pipeline_lasso, param_grid,\n                                cv = 5,\n                                scoring = 'neg_root_mean_squared_error',\n                                n_jobs = -1)\n\ngrid_search_lasso.fit(X_train, y_train)\n\nprint('Best Parameter: ', grid_search_lasso.best_params_)\nprint('Best CV RMSE: ', -grid_search_lasso.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:11.282594Z","iopub.execute_input":"2025-04-08T09:36:11.283026Z","iopub.status.idle":"2025-04-08T09:36:12.034085Z","shell.execute_reply.started":"2025-04-08T09:36:11.282991Z","shell.execute_reply":"2025-04-08T09:36:12.032653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hurray!!! We got the best value for the lasso for the parameter value 100","metadata":{}},{"cell_type":"markdown","source":"It's time to dive deep into over models, lets try the tree models and check how over model is reacting to them as well.","metadata":{}},{"cell_type":"markdown","source":"## Decision Tree Regressor\nRemember we have to create new pipeline for the data processing as planned let's encode the categorical variables with the label encoding and we don't need to standardize the data for numeric values.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeRegressor\n\n#pipeline for tree models for data processing\nnumeric_features_pipeline_tree = Pipeline(steps = [\n    ('Imputer', SimpleImputer(strategy = 'median')),\n])\n\ncategorical_features_pipeline_tree = Pipeline(steps=[\n    ('Imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('Ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\n#combine both \npreprocessor_tree = ColumnTransformer(transformers = [\n    ('numeric', numeric_features_pipeline_tree, numeric_features),\n    ('categorical', categorical_features_pipeline_tree, categorical_features)\n])\n\nmodel_pipeline_DT = Pipeline(steps = [\n    ('preprocessor', preprocessor_tree),\n    ('model', DecisionTreeRegressor())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:20.570225Z","iopub.execute_input":"2025-04-08T09:36:20.570678Z","iopub.status.idle":"2025-04-08T09:36:20.586503Z","shell.execute_reply.started":"2025-04-08T09:36:20.570634Z","shell.execute_reply":"2025-04-08T09:36:20.584938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Evaluate basline decision tree with cross validation score\nscores_DT = cross_val_score(model_pipeline_DT, X_train, y_train,\n                           cv = 5,\n                           scoring = 'neg_root_mean_squared_error')\n\nrmse_scores_DT = -scores_DT\nprint('RMSE score: ', rmse_scores_DT)\nprint('Avg RMSE: ', rmse_scores_DT.mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:24.113651Z","iopub.execute_input":"2025-04-08T09:36:24.114072Z","iopub.status.idle":"2025-04-08T09:36:24.358240Z","shell.execute_reply.started":"2025-04-08T09:36:24.114037Z","shell.execute_reply":"2025-04-08T09:36:24.357262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's tune a few key parameters:\n- max_depth: Maximum depth of the tree\n- min_samples_split: Minimum samples required to split an internal node\n- min_samples_leaf: Minimum samples required to be at a leaf node","metadata":{}},{"cell_type":"code","source":"param_grid_DT = {\n    'model__max_depth': [5, 10, 20, None],\n    'model__min_samples_split': [3, 7, 15],\n    'model__min_samples_leaf': [2, 4, 6]\n}\n\ngrid_search_DT = GridSearchCV(model_pipeline_DT, param_grid_DT,\n                             cv = 5,\n                             scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\ngrid_search_DT.fit(X_train, y_train)\n\nprint('Best Prameters', grid_search_DT.best_params_)\nprint('Best CV RMSE', grid_search_DT.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:26.776542Z","iopub.execute_input":"2025-04-08T09:36:26.776922Z","iopub.status.idle":"2025-04-08T09:36:30.143237Z","shell.execute_reply.started":"2025-04-08T09:36:26.776887Z","shell.execute_reply":"2025-04-08T09:36:30.142139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"model_pipeline_RF = Pipeline(steps = [\n    ('preprocessor', preprocessor_tree),\n    ('model', RandomForestRegressor())\n])\n\nparam_grid_RF = {\n    'model__n_estimators': [100, 200],  # Number of trees in the forest\n    'model__max_depth': [None, 10, 20],  # Max depth of each tree\n    'model__min_samples_split': [2, 5],  # Min samples to split an internal node\n    'model__min_samples_leaf': [1, 2],   # Min samples at a leaf node\n    'model__max_features': ['auto', 'sqrt']  # Number of features to consider at every split\n}\n\ngrid_search_RF = GridSearchCV(model_pipeline_RF, param_grid_RF,\n                             cv = 5,\n                             scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\ngrid_search_RF.fit(X_train, y_train)\n\nprint('Best Prameters', grid_search_RF.best_params_)\nprint('Best CV RMSE', grid_search_RF.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:36:47.804138Z","iopub.execute_input":"2025-04-08T09:36:47.804547Z","iopub.status.idle":"2025-04-08T09:38:00.616780Z","shell.execute_reply.started":"2025-04-08T09:36:47.804512Z","shell.execute_reply":"2025-04-08T09:38:00.615042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PERFECT WE GOT GOOD SCORE!! Now finally let's perform for the XGB","metadata":{}},{"cell_type":"markdown","source":"## XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"model_pipeline_XGB = Pipeline(steps = [\n    ('preprocessor', preprocessor_tree),\n    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n])\n\nparam_grid_XGB = {\n    'model__n_estimators': [100, 200],\n    'model__max_depth': [3, 5, 10],\n    'model__learning_rate': [0.01, 0.1, 0.3],\n    'model__subsample': [0.8, 1.0],\n    'model__colsample_bytree': [0.8, 1.0]\n}\n\ngrid_search_XGB = GridSearchCV(model_pipeline_XGB, param_grid_XGB,\n                             cv = 5,\n                             scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n\ngrid_search_XGB.fit(X_train, y_train)\n\nprint('Best Prameters', grid_search_XGB.best_params_)\nprint('Best CV RMSE', grid_search_XGB.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:39:46.056022Z","iopub.execute_input":"2025-04-08T09:39:46.056411Z","iopub.status.idle":"2025-04-08T09:41:04.300866Z","shell.execute_reply.started":"2025-04-08T09:39:46.056379Z","shell.execute_reply":"2025-04-08T09:41:04.299610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Refinement","metadata":{}},{"cell_type":"code","source":"#Use best parameters from GridSearch\nbest_rf_model = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=20,\n    max_features='sqrt',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    random_state=42\n)\n\n# Create pipeline again with best RF model\nmodel_pipeline_RF_final = Pipeline(steps=[\n    ('preprocessor', preprocessor_tree),\n    ('model', best_rf_model)\n])\n\n# Fit to train subset\nmodel_pipeline_RF_final.fit(X_train, y_train)\n\n# Predict on validation subset\nval_preds = model_pipeline_RF_final.predict(X_val)\n\n# Evaluate with RMSE\nfrom sklearn.metrics import mean_squared_error\n\nrmse_val = mean_squared_error(y_val, val_preds, squared=False)\nprint(\"Validation RMSE: \", rmse_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:42:58.930696Z","iopub.execute_input":"2025-04-08T09:42:58.931118Z","iopub.status.idle":"2025-04-08T09:42:59.326398Z","shell.execute_reply.started":"2025-04-08T09:42:58.931086Z","shell.execute_reply":"2025-04-08T09:42:59.325453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fit the model on original train set\nmodel_pipeline_RF_final.fit(X,y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:43:08.166991Z","iopub.execute_input":"2025-04-08T09:43:08.167365Z","iopub.status.idle":"2025-04-08T09:43:08.664718Z","shell.execute_reply.started":"2025-04-08T09:43:08.167338Z","shell.execute_reply":"2025-04-08T09:43:08.663717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#now it's time to bring our test set\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:43:15.485399Z","iopub.execute_input":"2025-04-08T09:43:15.485745Z","iopub.status.idle":"2025-04-08T09:43:15.504560Z","shell.execute_reply.started":"2025-04-08T09:43:15.485717Z","shell.execute_reply":"2025-04-08T09:43:15.502936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#good!! now let's predict the model on this test set\ntest_df_pred_log = model_pipeline_RF_final.predict(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:44:20.455343Z","iopub.execute_input":"2025-04-08T09:44:20.455773Z","iopub.status.idle":"2025-04-08T09:44:20.510414Z","shell.execute_reply.started":"2025-04-08T09:44:20.455740Z","shell.execute_reply":"2025-04-08T09:44:20.508602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.isnan(test_df_pred_log).sum(), \"NaNs in predictions\")\nprint(np.isinf(test_df_pred_log).sum(), \"Infs in predictions\")\nprint(\"Max value:\", np.max(test_df_pred_log))\nprint(\"Min value:\", np.min(test_df_pred_log))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:44:23.857169Z","iopub.execute_input":"2025-04-08T09:44:23.857537Z","iopub.status.idle":"2025-04-08T09:44:23.866454Z","shell.execute_reply.started":"2025-04-08T09:44:23.857507Z","shell.execute_reply":"2025-04-08T09:44:23.865426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df_preds = np.expm1(test_df_pred_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:44:29.460724Z","iopub.execute_input":"2025-04-08T09:44:29.461178Z","iopub.status.idle":"2025-04-08T09:44:29.467466Z","shell.execute_reply.started":"2025-04-08T09:44:29.461144Z","shell.execute_reply":"2025-04-08T09:44:29.465993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission DataFrame\nsubmission = pd.DataFrame({\n    'Id': test_df['Id'], \n    'SalePrice': test_df_preds\n})\n\n# Save to CSV (no index)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:44:44.996886Z","iopub.execute_input":"2025-04-08T09:44:44.997622Z","iopub.status.idle":"2025-04-08T09:44:45.009097Z","shell.execute_reply.started":"2025-04-08T09:44:44.997567Z","shell.execute_reply":"2025-04-08T09:44:45.007754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\nThe model performed well in the initial prediction. For improved results, we can focus on advanced feature engineering or fine-tuning the hyperparameters. However, it's important to ensure that the model does not overfit — training the model repeatedly without proper validation may cause it to memorize patterns instead of generalizing well.\n\nThanks for reading this notebook!  \nIf you need any help, feel free to contact me at: **dabidhussain2502@gmail.com**\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}